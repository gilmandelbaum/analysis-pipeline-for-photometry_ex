{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy  as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "mouse = 'T240'\n",
    "data_day = '2019_09_27__T240'\n",
    "date = '2019_09_27'\n",
    "\n",
    "HowManyBack = 1\n",
    "\n",
    "Nb2= 'a'\n",
    "#Nb3='b'\n",
    "\n",
    "data_dir_output = \"/Users/gilmandelbaum/Desktop/outPut\"\n",
    "\n",
    "seq_str = \"0a1b2a3h4ab5a6abc10a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(data_dir_output+\"/\"+mouse+\"/\"+data_day+'/'+str(HowManyBack)+\"_Back\")\n",
    "d = mouse+\"_\"+date+\"Notebook_2_\"+Nb2+'.pickle'\n",
    "my_path = root / d \n",
    "# open a file, where you stored the pickled data\n",
    "fileToOpen = open(my_path, 'rb')\n",
    "# dump information to that file\n",
    "aligned_behav_photo = pickle.load(fileToOpen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get df/f for photometry data based on sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_all_channel_afterDownSample = aligned_behav_photo[[\"d2 R\",\"d1 R\",\"d2 L\",\"d1 L\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_all_channel_afterDownSample_c = photometry_all_channel_afterDownSample.copy()\n",
    "photometry_all_channel_afterDownSample_c.to_dict('list');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/40084931/taking-subarrays-from-numpy-array-with-given-stride-stepsize/40085052#40085052\n",
    "# dang this is fast!\n",
    "def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n",
    "    nrows = ((a.size-L)//S)+1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S*n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(data, baseline_win, baseline_fun):\n",
    "\n",
    "    baseline_win_samples = int(baseline_win)\n",
    "    padded_data = np.pad(data,\n",
    "                         (baseline_win_samples // 2,\n",
    "                          baseline_win_samples // 2 - 1),\n",
    "                         'edge')\n",
    "\n",
    "    strides = strided_app(padded_data, baseline_win_samples, 1)\n",
    "    baseline = np.zeros((strides.shape[0], ), dtype='float32')\n",
    "    baseline[:] = np.nan\n",
    "\n",
    "    for k, data_slice in tqdm(enumerate(strides), total=strides.shape[0], desc='Computing baseline'):\n",
    "        use_data = data_slice[np.isfinite(data_slice)]\n",
    "        if len(use_data) > 0:\n",
    "            baseline[k] = baseline_fun(use_data)\n",
    "            \n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_fun = lambda x: np.percentile(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sig = {}\n",
    "\n",
    "for k, v in photometry_all_channel_afterDownSample_c.items():\n",
    "    baseline = get_baseline(v, 2000 ,baseline_fun)\n",
    "    final_sig['{}_f0'.format(k)] = baseline\n",
    "    final_sig['{}_dff'.format(k)] = (v - baseline) / baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_keys = [_ for _ in final_sig.keys() if 'dff' in _]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstamps = list(range(len(final_sig [\"d2 R_dff\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 10 seconds of photometry data to see how it looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(4, 1,figsize=(10,10),sharex=True, sharey=False)\n",
    "\n",
    "# for _ax, k in zip(ax.flatten(), use_keys):\n",
    "#     _ax.plot(tstamps, final_sig[k])\n",
    "#     _ax.set_title(k.split('_')[0])\n",
    "    \n",
    "# _ax.set_xlim(30000,40000)\n",
    "# _ax.set_xlabel('Time(ms)')\n",
    "# _ax.set_ylabel('dF/F0')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update photometry signal after df/f \n",
    "photometry_all_channel_df0_dff = pd.DataFrame(final_sig)\n",
    "photometry_all_channel_df0_dff = photometry_all_channel_df0_dff[[\"d2 R_dff\",\"d1 R_dff\",\"d2 L_dff\",\"d1 L_dff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualDffPercent = 100*photometry_all_channel_df0_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_behav_photo_dff = aligned_behav_photo\n",
    "aligned_behav_photo_dff[\"d2 R\"] = actualDffPercent[\"d2 R_dff\"].values\n",
    "aligned_behav_photo_dff[\"d1 R\"] = actualDffPercent[\"d1 R_dff\"].values\n",
    "aligned_behav_photo_dff[\"d2 L\"] = actualDffPercent[\"d2 L_dff\"].values\n",
    "aligned_behav_photo_dff[\"d1 L\"] = actualDffPercent[\"d1 L_dff\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = aligned_behav_photo_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smoothing_function (session):\n",
    "#     #make the window \n",
    "#     M = 200\n",
    "#     window = np.ones(M)\n",
    "#     window /= sum(window)\n",
    "    \n",
    "#     session_smooth = session.copy()\n",
    "    \n",
    "#     for channel_name in [\"d2 R\",\"d1 R\",\"d2 L\",\"d1 L\"]:\n",
    "        \n",
    "#         channel_data = session[channel_name]\n",
    "#         channel_data_padded = np.pad(channel_data,(M // 2, M // 2 - 1),'edge')\n",
    "#         session_smooth[channel_name+\"_smooth\"] = np.convolve(channel_data_padded,window,mode='valid')\n",
    "        \n",
    "#     return (session_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_function(session):\n",
    "    #make the window \n",
    "    M = 200\n",
    "    window = np.ones(M)\n",
    "    window /= sum(window)\n",
    "    \n",
    "    session_smooth = session.copy()\n",
    "    \n",
    "    for channel_name in [\"d2 R\",\"d1 R\",\"d2 L\",\"d1 L\"]:\n",
    "        \n",
    "        channel_data = session[channel_name]\n",
    "        channel_data_padded = np.pad(channel_data,(M // 2, M // 2 - 1),'edge')\n",
    "        session_smooth[channel_name] = np.convolve(channel_data_padded,window,mode='valid')\n",
    "        \n",
    "    return (session_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = smoothing_function(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# z_score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data [\"d2 R\"] = (processed_data[\"d2 R\"]-processed_data[\"d2 R\"].mean())/(processed_data[\"d2 R\"].std())\n",
    "# processed_data [\"d1 R\"] = (processed_data[\"d1 R\"]-processed_data[\"d1 R\"].mean())/(processed_data[\"d1 R\"].std())\n",
    "# processed_data [\"d2 L\"] = (processed_data[\"d2 L\"]-processed_data[\"d2 L\"].mean())/(processed_data[\"d2 L\"].std())\n",
    "# processed_data [\"d1 L\"] = (processed_data[\"d1 L\"]-processed_data[\"d1 L\"].mean())/(processed_data[\"d1 L\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the data after calculating the z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stash result in a cache directory, for how many back. \n",
    "cache_dir_plt = os.path.join(data_dir_output+'/'+mouse+'/'+data_day, str(HowManyBack)+'_Back',\"0.plots\")\n",
    "if not os.path.exists(cache_dir_plt):\n",
    "    os.makedirs(cache_dir_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_plot = cache_dir_plt+\"/\"+seq_str[:seq_str.index(\"4\")]+\"_plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"d2 R\",\"d1 R\",\"d2 L\",\"d1 L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1,figsize=(10,10),sharex=True, sharey=False)\n",
    "\n",
    "for _ax, k in zip(ax.flatten(), keys):\n",
    "    _ax.plot(tstamps, processed_data[k])\n",
    "    _ax.set_title(k.split('_')[0])\n",
    "    _ax.axhline(linewidth=1, color='r')\n",
    "_ax.set_xlim(30000,50000)\n",
    "_ax.set_xlabel('Time(ms)')\n",
    "_ax.set_ylabel(\"smoothed_df/f\")\n",
    "\n",
    "fig.savefig(path_to_plot+\"_1\"+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1,figsize=(10,10),sharex=True, sharey=False)\n",
    "\n",
    "for _ax, k in zip(ax.flatten(), keys):\n",
    "    _ax.plot(tstamps, processed_data[k])\n",
    "    _ax.set_title(k.split('_')[0])\n",
    "    _ax.axhline(linewidth=1, color='r')\n",
    "    \n",
    "_ax.set_xlim(80000,100000)\n",
    "_ax.set_xlabel('Time(ms)')\n",
    "_ax.set_ylabel(\"Zscore\")\n",
    "\n",
    "fig.savefig(path_to_plot+\"_2\"+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(data_dir_output+\"/\"+mouse+\"/\"+data_day+'/'+str(HowManyBack)+\"_Back\")\n",
    "d = mouse+\"_\"+date+\"Notebook_3_h\"+'_seq'+seq_str[seq_str.index(\"2\"):seq_str.index(\"4\")]+'.pickle'\n",
    "my_path = root / d \n",
    "my_file = open(my_path, 'wb')\n",
    "my_file = pickle.dump(processed_data,my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
